{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12e260170>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/Users/matthewnicastro/Desktop/reinforcement-learning/\")\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from models.MLP import MLP\n",
    "from utils.algorithms.ppo import proximal_policy_optimization\n",
    "from wrappers.training import ModelTrainingWrapper\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.device(\"mps\")\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CarRacing-v2\", domain_randomize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.85718924,  0.92844576,  0.39672935], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'gymnasium.envs.box2d.car_racing' from '/Users/matthewnicastro/Desktop/reinforcement-learning/.venv/lib/python3.11/site-packages/gymnasium/envs/box2d/car_racing.py'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.envs.box2d.car_racing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "policy_model = MLP(\n",
    "    architecture=[\n",
    "        (4, \"\", {}),\n",
    "        (64, \"ReLU\", {}),\n",
    "        (2, \"LogSoftmax\", {\"dim\": -1}),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def policy_output_parser(outputs, index=None):\n",
    "    if index is None:\n",
    "        actions = torch.multinomial(torch.exp(outputs), num_samples=1)\n",
    "    else:\n",
    "        actions = index.view(index.shape[-1], 1)\n",
    "    return actions, torch.gather(outputs, -1, index=actions).squeeze()\n",
    "\n",
    "\n",
    "policy_wrapper = ModelTrainingWrapper(\n",
    "    network=policy_model,\n",
    "    optimizer_name=\"Adam\",\n",
    "    optimizer_params={\"lr\": 3e-3},\n",
    "    output_parser=policy_output_parser,\n",
    ")\n",
    "value_model = MLP(\n",
    "    architecture=[\n",
    "        (4, \"\", {}),\n",
    "        (64, \"ELU\", {}),\n",
    "        (1, \"\", {}),\n",
    "    ]\n",
    ")\n",
    "value_wrapper = ModelTrainingWrapper(\n",
    "    network=value_model,\n",
    "    optimizer_name=\"Adam\",\n",
    "    optimizer_params={\"lr\": 1e-3},\n",
    "    output_parser=lambda output: output,\n",
    ")\n",
    "value_loss = torch.nn.MSELoss()\n",
    "\n",
    "logger = proximal_policy_optimization(\n",
    "    env=env,\n",
    "    state_parser=lambda state: torch.tensor(state, dtype=torch.float32),\n",
    "    policy_wrapper=policy_wrapper,\n",
    "    value_wrapper=value_wrapper,\n",
    "    value_loss=value_loss,\n",
    "    reward_func=lambda rewards, reward: 1 if reward > 0 else -2 * sum(rewards),\n",
    "    epochs=1000,\n",
    "    num_trajectories=10,\n",
    "    num_steps=500,\n",
    "    updates_per_epoch=10,\n",
    "    discount_factor=0.99,\n",
    "    gae_lambda=1.0,\n",
    "    clipping_parameter=0.2,\n",
    ")\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "# Plot the data on the subplots\n",
    "axs[0].plot(logger[\"policy_losses\"])\n",
    "axs[0].set_title(\"Policy Loss\")\n",
    "axs[1].plot(logger[\"value_losses\"])\n",
    "axs[1].set_title(\"Value Losses\")\n",
    "axs[2].plot(logger[\"num_steps\"])\n",
    "axs[2].set_title(\"Num steps over time\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "state = env.reset()[0]\n",
    "states = [state]\n",
    "probs = []\n",
    "done = False\n",
    "while not done:\n",
    "    state_t = torch.tensor(state)\n",
    "    action_prob = policy_wrapper.network(state_t)\n",
    "    probs.append(action_prob)\n",
    "    action, prob = policy_wrapper.output_parser(action_prob)\n",
    "    action = action.item()\n",
    "    new_state, reward, done, _, _ = env.step(action)\n",
    "    env.render()\n",
    "    states += [new_state]\n",
    "    state = new_state\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# sys.path.append(\"/Users/matthewnicastro/Desktop/reinforcement-learning/\")\n",
    "\n",
    "# import gymnasium as gym\n",
    "# import torch\n",
    "# from utils.io.model import load_model\n",
    "# from models.MLP import MLP\n",
    "\n",
    "# policy_wrapper = load_model(\n",
    "#     MLP, \"../weights/cart-pole-v1.pt\", \"../config/cart-pole-v1.pkl\", eval_mode=True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
